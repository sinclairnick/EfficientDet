{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/nick/Documents/school/research/EfficientLPR')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import svm\n",
    "from rapidfuzz import fuzz\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_l2_norm(y_trues, y_preds):\n",
    "    distances = []\n",
    "    for i in range(len(y_preds)): # calculate euclidian distance between preds and answer\n",
    "        y_true, y_pred = y_trues[i], y_preds[i]\n",
    "        distances.append(np.linalg.norm(y_true - y_pred))\n",
    "    return np.expand_dims(distances,1)\n",
    "\n",
    "def get_lev_distance(y_true, y_preds):\n",
    "    lev_distances = []\n",
    "    for row in np.hstack([np.expand_dims(y_preds, 1), np.expand_dims(y_true, 1)]):\n",
    "        lev_distances.append(fuzz.ratio(row[0], row[1]))\n",
    "    return np.expand_dims(lev_distances,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "LP Exact Accuracy: 0.74%\n"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script that transforms the predictions into three features:\n",
    "    lp_distance: a similarity metric representing the levenshtein distance between true and predicted LP\n",
    "    body_distance: distance between true and predicted body\n",
    "    color_distance: distance between true and predicted color\n",
    "\n",
    "\"\"\"\n",
    "test = True\n",
    "\n",
    "predictions_path = f'predictions_nzvd_{\"test\" if test else \"train\"}.full.csv'\n",
    "trues_path = f'data/processed/nzvd/{\"test\" if test else \"train\"}_annotations.csv'\n",
    "lps_path = f'data/raw/nzvd/{\"test\" if test else \"train\"}_labels.csv'\n",
    "classes_path = 'data/processed/classes.csv'\n",
    "colors_path = 'data/processed/colors.csv'\n",
    "\n",
    "# get class data\n",
    "classes = [x[0] for x in pd.read_csv(classes_path, header=None).values]\n",
    "colors = [x[0] for x in pd.read_csv(colors_path, header=None).values]\n",
    "class_labels = {x:i for i,x in enumerate(classes)}\n",
    "color_labels = {x:i for i,x in enumerate(colors)}\n",
    "\n",
    "# get y_pred \n",
    "preds = pd.read_csv(predictions_path)\n",
    "preds = preds.fillna('') # fill NaN values with empty string\n",
    "\n",
    "# get y_true\n",
    "lps = pd.read_csv(lps_path)[['lp-string']].T.squeeze()\n",
    "lps = lps.apply(lambda x: str(x).replace(' ', ''))\n",
    "trues = pd.read_csv(trues_path, header=None)\n",
    "if 'train' in trues_path:\n",
    "    trues = pd.concat([trues, pd.read_csv(trues_path.replace('train', 'val'), header=None)])\n",
    "trues.columns = ['file', 't', 'l', 'h', 'w', 'body', 'color']\n",
    "trues = trues.sort_values(by=['file'])\n",
    "trues.reset_index(inplace=True)\n",
    "trues = trues.assign(lp=lps)\n",
    "\n",
    "if False:\n",
    "    print(dict(zip(*np.unique(trues[['color']].values, return_counts=True))))\n",
    "    print(dict(zip(*np.unique(trues[['body']].values, return_counts=True))))\n",
    "\n",
    "# LICENSE PLATES    \n",
    "lp_true, lp_pred = trues[['lp']].values.squeeze(), preds[['lp']].values.squeeze()\n",
    "lp_acc = np.mean([lp_true == lp_pred])\n",
    "print(\"LP Exact Accuracy:\", f'{lp_acc}%', )\n",
    "\n",
    "def featurize(trues, preds):\n",
    "    \"\"\"Converts [preds, true] into [levenshtein distance, CCE_body, CCE_color]\"\"\"\n",
    "\n",
    "    # levenshtein distance of license plates\n",
    "    lev_distances = get_lev_distance(trues[['lp']].values.squeeze(), preds[['lp']].values.squeeze())\n",
    "\n",
    "    # BODY\n",
    "    body_true = list(map(lambda x: class_labels[x], trues[['body']].values.squeeze().tolist()))\n",
    "    body_true = tf.one_hot(body_true, depth=len(class_labels))\n",
    "    body_headers = [header for header in preds.columns if header.startswith('body')]\n",
    "    body_pred = preds[body_headers].values\n",
    "    body_cce = np.expand_dims(tf.losses.categorical_crossentropy(body_true, body_pred).numpy(), 1)\n",
    "\n",
    "    # COLOR\n",
    "    color_true = list(map(lambda x: color_labels[x], trues[['color']].values.squeeze().tolist()))\n",
    "    color_true = tf.one_hot(color_true, depth=len(color_labels))\n",
    "    color_headers = [header for header in preds.columns if header.startswith('color')]\n",
    "    color_pred = preds[color_headers].values\n",
    "    color_cce = np.expand_dims(tf.losses.categorical_crossentropy(color_true, color_pred).numpy(), 1)\n",
    "\n",
    "    # return [100-lev_distances, color_cce, body_cce] #full\n",
    "    # return [ color_cce, body_cce] #ablate lp\n",
    "    # return [100-lev_distances, color_cce] #ablate body\n",
    "    return [100-lev_distances, body_cce] #ablate color\n",
    "    # return [100-lev_distances] # ablate both\n",
    "    \n",
    "y_positive = np.expand_dims(np.repeat([1], len(preds)), 1) # positive samples have class==1\n",
    "x_positive = np.hstack(featurize(trues, preds))\n",
    "x_negative = np.empty((0,x_positive.shape[1]))\n",
    "y_negative = np.empty((0,1))\n",
    "\n",
    "# create negatives\n",
    "for i, sample in enumerate(trues.iloc):\n",
    "    \"\"\" Create negative samples. For each true sample, pair with every non-matching sample \"\"\"\n",
    "    headers = sample.index.values\n",
    "    neg_true = [sample.values for _ in range(len(trues)-1)]\n",
    "    neg_true = pd.DataFrame(neg_true, columns=headers)\n",
    "    # add all preds except current sample\n",
    "    neg_pred = pd.concat([preds.iloc[:i], preds.iloc[i+1:]])\n",
    "    neg_x = featurize(neg_true, neg_pred)\n",
    "    neg_x = np.hstack(neg_x)\n",
    "    neg_y = np.zeros((len(neg_x), 1))\n",
    "\n",
    "    x_negative = np.concatenate([x_negative, neg_x])\n",
    "    y_negative = np.concatenate([y_negative, neg_y])\n",
    "\n",
    "x_negative = np.array(x_negative)\n",
    "y_negative = np.array(y_negative)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# make some \"close\" samples where LD is the same, but body/color is wrong\n",
    "close_idxs = np.random.randint(0, len(x_positive), len(x_positive))\n",
    "x_close = np.hstack([x_positive[:,0].reshape(-1,1), x_negative[close_idxs,1:]])\n",
    "\n",
    "# balance data\n",
    "neg_idxs = np.random.randint(0, len(x_positive), len(x_positive))\n",
    "x_negative = x_negative[neg_idxs]\n",
    "y_negative = y_negative[neg_idxs]\n",
    "\n",
    "x1 = np.vstack([x_positive, x_negative])\n",
    "x2 = np.vstack([x_positive, x_close])\n",
    "y = np.vstack([y_positive, y_negative])\n",
    "\n",
    "scaler1 = MinMaxScaler().fit(x1)\n",
    "scaler2 = MinMaxScaler().fit(x2)\n",
    "\n",
    "x1 = scaler1.transform(x1)\n",
    "x2 = scaler2.transform(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Matched 97.0 %\nGating model coeffs [-6.84179102 -2.69650473]\nAssignment model coeffs [ 0.00838192 -4.77696708]\nCompared 100 instances to 99 others in 0.5820460319519043s\nUnmatched [['J66', 'UA7896', 'test/429.jpg'], ['', 'KQ2057', 'test/440.jpg'], ['LCJ513', 'LCJ510', 'test/471.jpg']]\n"
    }
   ],
   "source": [
    "import joblib\n",
    "if not test:\n",
    "    print('Training models')\n",
    "    \n",
    "    # classifies if in db\n",
    "    gater = LogisticRegression().fit(x1, y)\n",
    "\n",
    "    # picks which one\n",
    "    assigner = LogisticRegression().fit(x2, y)\n",
    "\n",
    "    joblib.dump(gater, 'logistic1.joblib')\n",
    "    joblib.dump(assigner, 'logistic2.joblib')\n",
    "\n",
    "else:\n",
    "    gater = joblib.load('logistic1.joblib')\n",
    "    assigner = joblib.load('logistic2.joblib')\n",
    "\n",
    "score_accs = []\n",
    "unmatched = []\n",
    "\n",
    "import time\n",
    "elapsed = 0\n",
    "\n",
    "# iterate over predictions, comparing each pred against whole GT database\n",
    "# if system works, the i-th GT entry should be the max fit\n",
    "for i, prediction in enumerate(preds.iloc):\n",
    "    \n",
    "    # initialize y_array\n",
    "    y_true = np.zeros(len(trues))\n",
    "    \n",
    "    # set single true match\n",
    "    y_true[i] = 1\n",
    "\n",
    "    # repeat sample\n",
    "    colnames = prediction.index.values\n",
    "    prediction = prediction.values\n",
    "    x_pred = [prediction for _ in range(len(trues))]\n",
    "    x_pred = pd.DataFrame(x_pred, columns=colnames)\n",
    "\n",
    "    s = time.time()\n",
    "    \n",
    "    # compare sample against all samples\n",
    "    x_pred = featurize(trues, x_pred)\n",
    "    x_pred = np.hstack(x_pred)\n",
    "\n",
    "    candidate_scores = gater.predict_proba(scaler1.transform(x_pred))[:,1]\n",
    "    proposals = np.where(candidate_scores > 0.5)[0]\n",
    "\n",
    "    # get proposal(s) by lowest LD\n",
    "    min_ld_idx = np.where(x_pred[:,0] == x_pred[:,0].min())\n",
    "    x_pred = x_pred[min_ld_idx]\n",
    "    y_cand = y_true[min_ld_idx]\n",
    "\n",
    "    candidate_scores = assigner.predict_proba(scaler2.transform(x_pred))[:,1]\n",
    "    max_score_idx = np.argmax(candidate_scores)\n",
    "    matched = y_cand[max_score_idx]\n",
    "    \n",
    "    pred_lp = prediction[-1]\n",
    "    true_lp = trues.iloc[i][['lp']].values[0]\n",
    "    fname = trues.iloc[i][['file']].values[0]\n",
    "\n",
    "    if not matched or len(proposals) < 1:\n",
    "        unmatched.append([pred_lp, true_lp, fname])\n",
    "        score_accs.append(False)\n",
    "    else:\n",
    "        score_accs.append(matched)\n",
    "    elapsed += time.time() - s\n",
    "\n",
    "    \n",
    "print('Matched', np.mean(score_accs)*100, '%')\n",
    "print('Gating model coeffs', gater.coef_[0])\n",
    "print('Assignment model coeffs', assigner.coef_[0])\n",
    "print('Compared {} instances to {} others in {}s'.format(len(preds), len(preds)-1, elapsed))\n",
    "print('Unmatched', unmatched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Precision 1.0\nRecall 0.97\nAccuracy 0.985\n"
    }
   ],
   "source": [
    "# testing only the gating model\n",
    "threshold = 0.5\n",
    "\n",
    "prec = tf.metrics.Precision()\n",
    "rec = tf.metrics.Recall()\n",
    "acc = tf.metrics.Accuracy()\n",
    "\n",
    "for x_, y_ in zip(x1, y):\n",
    "    if gater.predict_proba(np.expand_dims(x_, 0))[0,1] > threshold:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = 0\n",
    "    prec.update_state(y_,[p])\n",
    "    rec.update_state(y_,[p])\n",
    "    acc.update_state(y_,[p])\n",
    "    # if not p == y_:\n",
    "    #     print(x_, y_)\n",
    "print('Precision', prec.result().numpy())\n",
    "print('Recall', rec.result().numpy())\n",
    "print('Accuracy', acc.result().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Precision 0.61333334\nRecall 0.92\nAccuracy 0.67\n"
    }
   ],
   "source": [
    "# testing the assignment model\n",
    "# this is where the color and body needs to be useful\n",
    "prec = tf.metrics.Precision()\n",
    "rec = tf.metrics.Recall()\n",
    "acc = tf.metrics.Accuracy()\n",
    "for x_,y_ in zip(x2,y):\n",
    "    p = assigner.predict_proba(np.expand_dims(x_,0))[0,1] > 0.5\n",
    "    prec.update_state(y_,[p])\n",
    "    rec.update_state(y_,[p])\n",
    "    acc.update_state(y_,[p])\n",
    "\n",
    "print('Precision', prec.result().numpy())\n",
    "print('Recall', rec.result().numpy())\n",
    "print('Accuracy', acc.result().numpy())\n",
    "# this test effectively makes our logistic reg choose between two options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "d = pd.DataFrame(np.hstack([x[:size//2],y[:size//2]]), columns=['lp', 'color', 'body', 'match'])\n",
    "ax = sns.catplot(data=d[['lp', 'color', 'body']])\n",
    "ax.set(xlabel='feature', ylabel='normalized value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "d = pd.DataFrame(np.hstack([x[size//2:],y[size//2:]]), columns=['lp', 'color', 'body', 'match'])\n",
    "ax = sns.catplot(data=d[['lp', 'color', 'body']])\n",
    "ax.set(xlabel='feature', ylabel='normalized value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ignore below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "scores = []\n",
    "\n",
    "# step 1: assuming body == color weighting, adjust LP weighting to find highest accuracy\n",
    "k_values = np.linspace(0, 5, 51)\n",
    "for j in k_values:\n",
    "    score_accs = []\n",
    "\n",
    "    # compare each prediction against entire GT \"database\"\n",
    "    for i, prediction in enumerate(preds.iloc):\n",
    "        # initialize y_array\n",
    "        y_true = np.zeros(len(trues))\n",
    "        # set single true match\n",
    "        y_true[i] = 1\n",
    "        # repeat sample\n",
    "        colnames = prediction.index.values\n",
    "        prediction = prediction.values\n",
    "        x_pred = [prediction for _ in range(len(trues))]\n",
    "        x_pred = pd.DataFrame(x_pred, columns=colnames)\n",
    "\n",
    "        # compare sample against all samples\n",
    "        x_pred = featurize(trues, x_pred)\n",
    "        x_pred = np.hstack(x_pred)\n",
    "\n",
    "        score = similarity(x_pred, [j, 1, 1])\n",
    "        imax = np.argmax(score)\n",
    "        score_accs.append(y_true[imax] == 1)\n",
    "    scores.append(np.mean(score_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2.8 # best k factor\n",
    "\n",
    "# step 2: see which threshold gets optimal accuracy\n",
    "thresholds = np.linspace(0, k, k*10+1)\n",
    "metrics = np.zeros(shape=(len(thresholds), 3))\n",
    "\n",
    "for threshold in thresholds:\n",
    "    prec = tf.metrics.Precision()\n",
    "    rec = tf.metrics.Recall()\n",
    "    acc = tf.metrics.Accuracy()\n",
    "    for x_, y_ in zip(x, y):\n",
    "        if similarity(x_, [k, 1, 1]) > threshold:\n",
    "            prec.update_state(y_,[1])\n",
    "            rec.update_state(y_,[1])\n",
    "            acc.update_state(y_,[1])\n",
    "        else:\n",
    "            prec.update_state(y_,[0])\n",
    "            rec.update_state(y_,[0])\n",
    "            acc.update_state(y_,[0])\n",
    "    index = int(threshold*10)\n",
    "    metrics[index,0] = prec.result().numpy()\n",
    "    metrics[index,1] = rec.result().numpy()\n",
    "    metrics[index,2] = acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: establish impact of color and body, need to compare against many thresholds\n",
    "thresholds = np.linspace(0,5,51)\n",
    "res = np.zeros(shape=(len(thresholds),2,2))\n",
    "\n",
    "for threshold in thresholds:\n",
    "    for b_color in [0,1]:   \n",
    "        for b_body in [0,1]:\n",
    "            prec = tf.metrics.Precision()\n",
    "            rec = tf.metrics.Recall()\n",
    "            acc = tf.metrics.Accuracy() \n",
    "            for x_, y_ in zip(x, y):\n",
    "                if similarity(x_, [k, b_color, b_body]) > threshold:\n",
    "                    prec.update_state(y_,[1])\n",
    "                    rec.update_state(y_,[1])\n",
    "                    acc.update_state(y_,[1])\n",
    "                else:\n",
    "                    prec.update_state(y_,[0])\n",
    "                    rec.update_state(y_,[0])\n",
    "                    acc.update_state(y_,[0])\n",
    "            res[int(threshold*10), b_color, b_body] = acc.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores, columns=['acc'])\n",
    "\n",
    "ax = sns.lineplot(data=scores_df)\n",
    "ax.set(xlabel='k', ylabel='acc')\n",
    "ax.set_xticklabels([0,0, 1, 2, 3, 4, 5])\n",
    "plt.show()\n",
    "i_max_score = np.argmax(scores_df[\"acc\"])\n",
    "print('Score acc maximized at:', i_max_score/10)\n",
    "print(scores_df.iloc[i_max_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics, columns=['precision', 'recall', 'accuracy'])\n",
    "ax = sns.lineplot(data=metrics_df)\n",
    "ax.set_xticklabels([0,0,1,2,3,4,5])\n",
    "ax.set(xlabel=\"threshold\")\n",
    "plt.show()\n",
    "i_max_acc = np.argmax(metrics_df[\"accuracy\"])\n",
    "print('Max acc idx:', i_max_acc/10)\n",
    "print(metrics_df.iloc[i_max_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# illustrate influence of the ancillary features\n",
    "# (0,0) cell shows accuracy without body and color\n",
    "for confusion in res:\n",
    "    print(confusion[1,1], confusion[0,0])\n",
    "res_df = pd.DataFrame(res)\n",
    "sns.heatmap(data=res, annot=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitanaconda3virtualenvc7eb3ab0ecc14188bea54639f0fd4b91",
   "display_name": "Python 3.7.6 64-bit ('anaconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}